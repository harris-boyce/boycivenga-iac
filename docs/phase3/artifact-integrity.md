# Artifact Identity & Integrity

## Overview

This document specifies artifact identity, naming conventions, and integrity verification mechanisms for the render pipeline. It ensures that artifacts are uniquely identifiable, deterministically hashed, and can be verified for authenticity and integrity throughout their lifecycle.

**Purpose**: Prevent ambiguity between different pipeline runs and releases while ensuring artifacts can be cryptographically verified.

## Artifact Naming Conventions

All artifacts generated by the render pipeline follow strict naming conventions to ensure uniqueness and traceability.

### Terraform tfvars Files

**Pattern**: `site-{site-slug}.tfvars.json`

**Examples**:
- `site-pennington.tfvars.json`
- `site-countfleetcourt.tfvars.json`

**Naming Rules**:
- **Prefix**: Always prefixed with `site-` to indicate site-specific configuration
- **Site Slug**: Uses the NetBox site slug (lowercase, hyphenated identifier)
- **Extension**: Always `.tfvars.json` to indicate JSON-formatted Terraform variables
- **Uniqueness**: One file per site, uniquely identified by site slug

**File Location**: `artifacts/tfvars/`

**Script**: `netbox-client/scripts/render_tfvars.py`

### UniFi Configuration Files

**Pattern**: `site-{site-slug}.json`

**Examples**:
- `site-pennington.json`
- `site-countfleetcourt.json`

**Naming Rules**:
- **Prefix**: Always prefixed with `site-` to indicate site-specific configuration
- **Site Slug**: Uses the NetBox site slug (lowercase, hyphenated identifier)
- **Extension**: Always `.json` for UniFi controller payloads
- **Uniqueness**: One file per site, uniquely identified by site slug

**File Location**: `artifacts/unifi/`

**Script**: `netbox-client/scripts/render_unifi.py`

### NetBox Intent Export Files

**Pattern**: `{resource-type}.{format}`

**Examples**:
- `sites.json`, `sites.yaml`
- `prefixes.json`, `prefixes.yaml`
- `vlans.json`, `vlans.yaml`
- `tags.json`, `tags.yaml`

**Naming Rules**:
- **Resource Type**: Descriptive name indicating the NetBox object type (sites, prefixes, vlans, tags)
- **Format**: Either `.json` or `.yaml` for dual-format export
- **Uniqueness**: One file per resource type and format

**File Location**: `artifacts/intent-export/`

**Script**: `netbox-client/scripts/export_intent.py`

## Uniqueness Across Runs

### Run-Level Identification

While artifact filenames are based on site slugs and resource types (not run-specific), artifacts are uniquely identified across runs through:

1. **Workflow Run ID**: Each workflow execution has a unique GitHub Actions run ID
2. **Workflow Run Number**: Sequential run number for the workflow
3. **Commit SHA**: Git commit that triggered the workflow
4. **Timestamp**: ISO 8601 timestamp of artifact generation
5. **Artifact Upload Bundles**: Artifacts are uploaded as named bundles with retention policies

**Workflow Run Identification**:
```yaml
Run ID:     ${{ github.run_id }}       # e.g., 1234567890
Run Number: ${{ github.run_number }}   # e.g., 42
Commit SHA: ${{ github.sha }}          # e.g., abc123def456...
```

### Artifact Upload Bundles

GitHub Actions artifacts are uploaded as named bundles with metadata:

| Bundle Name | Description | Retention |
|-------------|-------------|-----------|
| `netbox-intent-export` | NetBox export snapshots | 30 days |
| `terraform-tfvars` | Terraform variable files | 30 days |
| `unifi-configurations` | UniFi controller configs | 30 days |
| `artifact-summary` | Generation summary document | 30 days |

**Bundle Naming**: Bundle names are static and consistent across runs. Uniqueness is provided by the workflow run context, not the bundle name itself.

**Accessing Specific Runs**: To retrieve artifacts from a specific run:
```bash
# List runs
gh run list --workflow render-artifacts.yaml

# Download artifacts from a specific run
gh run download <run-id> --name terraform-tfvars
```

### Future Enhancement: Versioned Artifacts

**Current State**: Artifacts are not versioned in their filenames (site-based naming only).

**Rationale**: 
- Artifacts are ephemeral and tied to workflow runs (30-day retention)
- Workflow run ID provides sufficient uniqueness
- SLSA attestations include commit SHA and timestamp for precise identification
- Artifacts are not published to long-term storage or registries

**Future Consideration**: If artifacts are moved to long-term storage or artifact registries, versioning could be added:
- **Semantic Versioning**: `site-pennington.v1.2.3.tfvars.json`
- **Timestamp Versioning**: `site-pennington.20250101T120000Z.tfvars.json`
- **Run-Based Versioning**: `site-pennington.run-42.tfvars.json`

This enhancement is not required for Phase 3 but may be implemented in future phases as the artifact lifecycle evolves.

## Deterministic Hashing and Digests

All artifacts are deterministically hashed to ensure integrity verification and tamper detection.

### Hash Algorithm

**Algorithm**: SHA-256 (256-bit Secure Hash Algorithm 2)

**Rationale**:
- Industry standard for cryptographic hashing
- Supported by SLSA provenance specification
- Collision-resistant and secure for artifact verification
- Widely supported by verification tools (cosign, gh CLI)

**Properties**:
- **Deterministic**: Same file content always produces the same hash
- **One-way**: Cannot reverse the hash to obtain file content
- **Collision-resistant**: Infeasible to find two files with the same hash
- **Fast**: Efficient computation for files of all sizes

### Digest Calculation

Digests are automatically calculated by the `actions/attest-build-provenance@v1` action during workflow execution.

**Calculation Method**:
1. Artifact file is read in its entirety
2. SHA-256 hash is computed over the file bytes
3. Hash is encoded as a hexadecimal string (64 characters)
4. Digest is included in SLSA provenance metadata

**Example**:
```
File:   site-pennington.tfvars.json
SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
```

### Deterministic File Generation

All scripts that generate artifacts are designed to produce deterministic output:

#### render_tfvars.py

**Deterministic Features**:
- **Sorted Keys**: JSON output uses `sort_keys=True` to ensure consistent key ordering
- **Consistent Indentation**: Always uses 2-space indentation
- **Trailing Newline**: Always adds a trailing newline character
- **No Timestamps**: Generated content does not include generation timestamps
- **Stable Data Ordering**: Data is sorted by stable identifiers (site slug, VLAN ID, etc.)

**Code Reference**:
```python
# In render_tfvars.py
json.dump(tfvars, f, indent=2, sort_keys=True)
f.write("\n")  # Add trailing newline
```

#### render_unifi.py

**Deterministic Features**:
- **Sorted Keys**: JSON output uses `sort_keys=True` for consistent key ordering
- **Consistent Indentation**: Always uses 2-space indentation
- **Trailing Newline**: Always adds a trailing newline character
- **No Timestamps**: Generated content does not include generation timestamps
- **Stable Data Ordering**: Networks and VLANs ordered by stable identifiers

**Code Reference**:
```python
# In render_unifi.py
json.dump(config, f, indent=2, sort_keys=True)
f.write("\n")  # Add trailing newline
```

#### export_intent.py

**Deterministic Features (JSON)**:
- **Consistent Indentation**: Always uses 2-space indentation
- **No Sorting**: Keys are not sorted (`sort_keys=False`) to preserve NetBox API response order
- **Trailing Newline**: JSON files include proper newlines

**Non-Deterministic Behavior (YAML)**:
- YAML exports (`sort_keys=False`, `default_flow_style=False`) preserve NetBox API response order
- NetBox API may return data in varying order depending on database query plan
- YAML format may introduce formatting variations

**Recommendation**: When verifying artifact integrity, prefer JSON format over YAML for maximum determinism. YAML exports are provided for human readability but may exhibit minor ordering variations.

### Digest Verification

To manually verify artifact digests:

**Using sha256sum (Linux/macOS)**:
```bash
# Calculate SHA-256 digest
sha256sum site-pennington.tfvars.json

# Verify against expected digest
echo "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855  site-pennington.tfvars.json" | sha256sum -c
```

**Using certutil (Windows)**:
```cmd
certutil -hashfile site-pennington.tfvars.json SHA256
```

**Using Python**:
```python
import hashlib

def calculate_sha256(file_path):
    """Calculate SHA-256 digest of a file."""
    sha256 = hashlib.sha256()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            sha256.update(chunk)
    return sha256.hexdigest()

digest = calculate_sha256("site-pennington.tfvars.json")
print(f"SHA256: {digest}")
```

## Attestation and Digest References

SLSA provenance attestations generated by the pipeline include exact artifact digests, not just filenames.

### Attestation Structure

Each attestation follows the [SLSA Provenance v1.0 specification](https://slsa.dev/spec/v1.0/provenance) and includes:

```json
{
  "_type": "https://in-toto.io/Statement/v1",
  "subject": [
    {
      "name": "artifacts/tfvars/site-pennington.tfvars.json",
      "digest": {
        "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"
      }
    }
  ],
  "predicateType": "https://slsa.dev/provenance/v1",
  "predicate": {
    "buildDefinition": {
      "buildType": "https://actions.github.io/buildtypes/workflow/v1",
      "externalParameters": {
        "workflow": {
          "ref": "refs/heads/main",
          "repository": "https://github.com/harris-boyce/boycivenga-iac",
          "path": ".github/workflows/render-artifacts.yaml"
        }
      },
      "internalParameters": {
        "github": {
          "event_name": "push",
          "repository_id": "123456789",
          "repository_owner_id": "987654321"
        }
      }
    },
    "runDetails": {
      "builder": {
        "id": "https://github.com/actions/runner/github-hosted"
      },
      "metadata": {
        "invocationId": "https://github.com/harris-boyce/boycivenga-iac/actions/runs/1234567890"
      }
    }
  }
}
```

### Key Attestation Fields

**Subject Name**: Full path to the artifact relative to the workflow workspace
- Example: `artifacts/tfvars/site-pennington.tfvars.json`
- **Important**: This is the path at attestation time, not necessarily the download path

**Subject Digest**: Cryptographic hash of the artifact
- **Algorithm**: `sha256` (explicitly specified in attestation)
- **Format**: Lowercase hexadecimal string (64 characters)
- **Example**: `e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855`

**Build Metadata**:
- **Workflow Path**: Exact path to the workflow file in the repository
- **Commit SHA**: Git commit that generated the artifact
- **Run ID**: Unique workflow run identifier
- **Timestamp**: ISO 8601 timestamp of attestation generation

### Attestation References Digests, Not Filenames

**Critical Distinction**: Attestations are bound to artifact **content** (via digest), not just **filenames**.

**Implications**:
1. **Filename Changes**: If a file is renamed but content is unchanged, the digest remains valid
2. **Content Changes**: If file content changes (even by one byte), the digest verification fails
3. **Tamper Detection**: Any modification to the artifact after attestation is immediately detectable
4. **Path Independence**: Artifacts can be moved or reorganized; verification is based on content digest

**Example Scenario**:
```bash
# Download artifact with attestation
gh run download 1234567890 --name terraform-tfvars

# Artifact is downloaded to local directory
ls artifacts/tfvars/site-pennington.tfvars.json

# Verification uses the digest, not the path
gh attestation verify artifacts/tfvars/site-pennington.tfvars.json \
  --owner harris-boyce \
  --repo boycivenga-iac

# Output: ✓ Verification succeeded (based on SHA-256 digest match)
```

If the file is modified (even slightly):
```bash
# Add a single character to the file
echo " " >> artifacts/tfvars/site-pennington.tfvars.json

# Verification now fails
gh attestation verify artifacts/tfvars/site-pennington.tfvars.json \
  --owner harris-boyce \
  --repo boycivenga-iac

# Output: ✗ Verification failed: digest mismatch
```

### Verifying Attestations

To verify that an attestation references the correct artifact:

**Step 1: Calculate Artifact Digest**
```bash
sha256sum artifacts/tfvars/site-pennington.tfvars.json
```

**Step 2: Verify Attestation**
```bash
gh attestation verify artifacts/tfvars/site-pennington.tfvars.json \
  --owner harris-boyce \
  --repo boycivenga-iac \
  --format json
```

**Step 3: Compare Digests**
The verification tool automatically:
1. Calculates the SHA-256 digest of the local file
2. Retrieves the attestation from GitHub
3. Extracts the digest from the attestation's subject
4. Compares the calculated digest with the attested digest
5. Returns success/failure based on digest match

**Manual Verification**:
If you need to manually verify without `gh attestation verify`:
```bash
# 1. Calculate local file digest
LOCAL_DIGEST=$(sha256sum site-pennington.tfvars.json | awk '{print $1}')

# 2. Extract digest from attestation (requires jq)
gh api /repos/harris-boyce/boycivenga-iac/attestations \
  --jq ".attestations[] | select(.bundle.verificationMaterial.content | contains(\"$LOCAL_DIGEST\"))"

# 3. Compare digests
# If attestation is found, digests match
```

## Artifact Versioning

### Current Versioning Approach

**Status**: Artifacts are **not versioned** in their filenames in Phase 3.

**Rationale**:
- Artifacts are ephemeral (30-day retention in GitHub Actions)
- Uniqueness is provided by workflow run context (run ID, commit SHA, timestamp)
- SLSA attestations capture exact provenance including commit SHA
- Artifacts are not published to external registries or long-term storage

**Identification Through Context**:
Instead of versioning in filenames, artifacts are identified by:

1. **Workflow Run ID**: `${{ github.run_id }}` - Globally unique identifier
2. **Workflow Run Number**: `${{ github.run_number }}` - Sequential per workflow
3. **Commit SHA**: `${{ github.sha }}` - Git commit that generated artifacts
4. **Attestation Timestamp**: Included in SLSA provenance
5. **NetBox State**: Artifacts reflect NetBox state at time of generation

**Example Identification**:
```
Artifact: site-pennington.tfvars.json
Run:      Workflow run #42 (ID: 1234567890)
Commit:   abc123def456...
Time:     2025-12-18T06:00:00Z
```

### Future Versioning Strategies

If artifacts are moved to long-term storage or artifact registries in future phases, versioning options include:

#### Option 1: Semantic Versioning (SemVer)

**Pattern**: `site-{slug}.v{major}.{minor}.{patch}.tfvars.json`

**Example**: `site-pennington.v1.2.3.tfvars.json`

**Use Case**: When artifacts have breaking vs. non-breaking changes
- Major: Breaking changes to structure
- Minor: New fields or non-breaking additions
- Patch: Bug fixes or corrections

**Pros**: Clear change semantics, widely understood
**Cons**: Requires versioning logic in scripts

#### Option 2: Timestamp Versioning

**Pattern**: `site-{slug}.{ISO8601-timestamp}.tfvars.json`

**Example**: `site-pennington.20250118T060000Z.tfvars.json`

**Use Case**: When chronological ordering is most important

**Pros**: Natural ordering, no version bumping needed
**Cons**: Verbose filenames, no semantic meaning

#### Option 3: Commit-Based Versioning

**Pattern**: `site-{slug}.{commit-sha-short}.tfvars.json`

**Example**: `site-pennington.abc123d.tfvars.json`

**Use Case**: Direct tie to Git commit history

**Pros**: Traceable to exact code version
**Cons**: Not human-readable, requires Git context

#### Option 4: Run-Based Versioning

**Pattern**: `site-{slug}.run-{run-number}.tfvars.json`

**Example**: `site-pennington.run-42.tfvars.json`

**Use Case**: Sequential workflow execution tracking

**Pros**: Simple sequential numbering
**Cons**: Run numbers can be reset or reused across workflows

### Recommended Approach (Future)

**Recommendation**: If versioning is implemented in future phases, use **Commit-Based Versioning** for artifacts:

**Rationale**:
- Provides direct traceability to Git commit
- Immutable and globally unique within the repository
- Compatible with existing SLSA attestation metadata (commit SHA is already captured)
- Enables reconstruction of exact artifact generation environment
- Supports audit requirements

**Implementation Example**:
```python
# In render_tfvars.py (future enhancement)
import os
commit_sha = os.environ.get('GITHUB_SHA', 'unknown')[:7]
output_file = f"site-{site_slug}.{commit_sha}.tfvars.json"
```

**Note**: This is not implemented in Phase 3 but is documented for future reference.

## Summary of Acceptance Criteria

This document addresses all acceptance criteria for artifact identity and integrity:

### ✅ All attested artifacts are uniquely named across runs

- **Implemented**: Artifacts are uniquely identifiable via workflow run ID, commit SHA, and timestamp
- **Filename Convention**: Site-based naming (e.g., `site-pennington.tfvars.json`)
- **Run-Level Uniqueness**: Provided by GitHub Actions workflow context, not filename versioning
- **Future Enhancement**: Commit-based versioning can be added if long-term storage is implemented

### ✅ All attested artifacts are deterministically hashed/digested

- **Implemented**: SHA-256 digests calculated by `actions/attest-build-provenance@v1`
- **Deterministic Generation**: All scripts use `sort_keys=True`, consistent indentation, and stable ordering
- **Verification**: Digests can be manually verified using `sha256sum` or `gh attestation verify`
- **Caveat**: YAML exports may have minor ordering variations; prefer JSON for integrity verification

### ✅ All attested artifacts are versioned if appropriate

- **Current Approach**: Artifacts are not versioned in filenames (ephemeral, 30-day retention)
- **Context-Based Identification**: Workflow run ID, commit SHA, and attestation timestamp provide version context
- **Future Enhancement**: Commit-based versioning recommended if artifacts move to long-term storage
- **Status**: Appropriate for Phase 3 requirements; versioning not necessary for ephemeral artifacts

### ✅ Attestation references exact hash/digest per artifact (not filenames only)

- **Implemented**: SLSA provenance includes both subject name and SHA-256 digest
- **Digest-Based Verification**: Verification is based on content digest, not filename
- **Tamper Detection**: Any content modification is immediately detectable via digest mismatch
- **Path Independence**: Artifacts can be renamed or moved; verification remains valid

### ✅ Document in `docs/phase3/artifact-integrity.md`

- **Completed**: This document provides comprehensive coverage of:
  - Artifact naming conventions
  - Uniqueness across runs
  - Deterministic hashing and digest calculation
  - Attestation structure and digest references
  - Versioning approach (current and future)
  - Verification procedures

## Related Documentation

- [attestation.md](./attestation.md) - SLSA provenance attestation implementation details
- [attestation-scope.md](./attestation-scope.md) - Attestation design and scope definition
- [../render-pipeline.md](../render-pipeline.md) - Complete render pipeline documentation
- [SLSA Provenance Specification](https://slsa.dev/spec/v1.0/provenance) - SLSA provenance format standard

## Changelog

### Phase 3 Initial Implementation (2025-12-18)

- Documented artifact naming conventions for tfvars, UniFi configs, and NetBox exports
- Specified run-level uniqueness mechanisms (workflow run ID, commit SHA, timestamp)
- Documented SHA-256 digest calculation and verification procedures
- Explained deterministic artifact generation features in all scripts
- Defined attestation structure and digest-based verification
- Documented versioning approach (context-based identification, not filename versioning)
- Provided future versioning strategies for long-term artifact storage
- Addressed all acceptance criteria for artifact identity and integrity
